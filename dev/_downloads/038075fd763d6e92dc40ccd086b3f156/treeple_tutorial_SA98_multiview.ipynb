{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# 2-1a: Calculating S@98 with multiview data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.metrics import roc_curve\n\nfrom sktree.datasets import make_trunk_classification\nfrom sktree.ensemble import HonestForestClassifier\nfrom sktree.stats import build_hyppo_oob_forest\nfrom sktree.tree import MultiViewDecisionTreeClassifier\n\nsns.set(color_codes=True, style=\"white\", context=\"talk\", font_scale=1.5)\nPALETTE = sns.color_palette(\"Set1\")\nsns.set_palette(PALETTE[1:5] + PALETTE[6:], n_colors=9)\nsns.set_style(\"white\", {\"axes.edgecolor\": \"#dddddd\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## S@98 with multiview data\n\nSensitivity at 98% specificity (*S@98*) measures, namely, the true\npositive rate (*TPR*) when the false positive rate (*FPR*) is at 98%.\n\n\\begin{align}S@r = \\mathbb{P}[\\eta(X) > T_r \\mid Y=1]\\end{align}\n\nWith a multiview binary class simulation as an example, this tutorial\nwill show how to use ``treeple`` to calculate the statistic with\nmultiview data. For data with a single feature set, you can check out\nthe simpler S@98 tutorial.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create a two-dimensional simulation with gaussians\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# create a binary class simulation with two gaussians\n# 500 samples for each class, class zero is standard\n# gaussian, and class one has a mean at one\nZ, y = make_trunk_classification(\n    n_samples=1000,\n    n_dim=1,\n    mu_0=0,\n    mu_1=1,\n    n_informative=1,\n    seed=1,\n)\n\n# class one has a mean at two for X\nX, y = make_trunk_classification(\n    n_samples=1000,\n    n_dim=1,\n    mu_0=0,\n    mu_1=2,\n    n_informative=1,\n    seed=2,\n)\n\nZ_X = np.hstack((Z, X))\n\n\nZ_X_y = np.hstack((Z_X, y.reshape(-1, 1)))\nZ_X_y = pd.DataFrame(Z_X_y, columns=[\"Z\", \"X\", \"y\"])\nZ_X_y = Z_X_y.replace({\"y\": 0.0}, \"Class Zero\")\nZ_X_y = Z_X_y.replace({\"y\": 1.0}, \"Class One\")\n\nfig, ax = plt.subplots(figsize=(6, 6))\nfig.tight_layout()\nax.tick_params(labelsize=15)\nsns.scatterplot(data=Z_X_y, x=\"Z\", y=\"X\", hue=\"y\", palette=PALETTE[:2], alpha=0.2)\nsns.kdeplot(data=Z_X_y, x=\"Z\", y=\"X\", hue=\"y\", palette=PALETTE[:2], alpha=0.6)\nax.set_ylabel(\"X\", fontsize=15)\nax.set_xlabel(\"Z\", fontsize=15)\nplt.legend(frameon=False, fontsize=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fit the model with X and Z\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# initialize the forest with 100 trees\nest = HonestForestClassifier(\n    n_estimators=100,\n    max_samples=1.6,\n    max_features=0.3,\n    bootstrap=True,\n    stratify=True,\n    tree_estimator=MultiViewDecisionTreeClassifier(),\n    random_state=1,\n)\n\n# fit the model and obtain the tree posteriors\n_, observe_proba = build_hyppo_oob_forest(est, Z_X, y)\n\n# generate forest posteriors for the two classes\nobserve_proba = np.nanmean(observe_proba, axis=0)\n\n\nfig, ax = plt.subplots(figsize=(6, 6))\nfig.tight_layout()\nax.tick_params(labelsize=15)\n\n# histogram plot the posterior probabilities for class one\nax.hist(observe_proba[:500][:, 1], bins=50, alpha=0.6, color=PALETTE[1], label=\"negative\")\nax.hist(observe_proba[500:][:, 1], bins=50, alpha=0.3, color=PALETTE[0], label=\"positive\")\nax.set_ylabel(\"# of Samples\", fontsize=15)\nax.set_xlabel(\"Class One Posterior\", fontsize=15)\nplt.legend(frameon=False, fontsize=15)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calculate the statistic\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def Calculate_SA(y_true, y_pred_proba, max_fpr=0.02) -> float:\n    \"\"\"Calculate the sensitivity at a specific specificity\"\"\"\n    # check the shape of true labels\n    if y_true.squeeze().ndim != 1:\n        raise ValueError(f\"y_true must be 1d, not {y_true.shape}\")\n\n    # find the positive class and calculate fpr and tpr\n    if 0 in y_true or -1 in y_true:\n        fpr, tpr, thresholds = roc_curve(\n            y_true, y_pred_proba[:, 1], pos_label=1, drop_intermediate=False\n        )\n    else:\n        fpr, tpr, thresholds = roc_curve(\n            y_true, y_pred_proba[:, 1], pos_label=2, drop_intermediate=False\n        )\n    sa98 = max([tpr for (fpr, tpr) in zip(fpr, tpr) if fpr <= max_fpr])\n\n    fig, ax = plt.subplots(figsize=(6, 6))\n    fig.tight_layout()\n    ax.tick_params(labelsize=15)\n    ax.set_xlim([-0.005, 1.005])\n    ax.set_ylim([-0.005, 1.005])\n    ax.set_xlabel(\"False Positive Rate\", fontsize=15)\n    ax.set_ylabel(\"True Positive Rate\", fontsize=15)\n\n    ax.plot(fpr, tpr, label=\"ROC curve\", color=PALETTE[1])\n\n    spec = int((1 - max_fpr) * 100)\n    ax.axvline(\n        x=max_fpr,\n        color=PALETTE[0],\n        ymin=0,\n        ymax=sa98,\n        label=\"S@\" + str(spec) + \" = \" + str(round(sa98, 2)),\n        linestyle=\"--\",\n    )\n    ax.axhline(y=sa98, xmin=0, xmax=max_fpr, color=\"r\", linestyle=\"--\")\n    ax.legend(frameon=False, fontsize=15)\n\n    return sa98\n\n\nsa98 = Calculate_SA(y, observe_proba, max_fpr=0.02)\nprint(\"S@98 =\", round(sa98, 2))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}