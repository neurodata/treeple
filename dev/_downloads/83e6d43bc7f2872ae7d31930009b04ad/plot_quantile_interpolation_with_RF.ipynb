{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Predicting with different quantile interpolation methods\n\nAn example comparison of interpolation methods that can be applied during\nprediction when the desired quantile lies between two data points.\n\nThis example was heavily inspired by [quantile-forest](https://github.com/zillow/quantile-forest)\npackage. See their package [here](https://zillow.github.io/quantile-forest/).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate the data\nWe use four simple data points to illustrate the difference between the intervals that are\ngenerated using different interpolation methods.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X = np.array([[-1, -1], [-1, -1], [-1, -1], [1, 1], [1, 1]])\ny = np.array([-2, -1, 0, 1, 2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The interpolation methods\nThe following interpolation methods demonstrated here are:\nTo interpolate between the data points, i and j (``i <= j``),\nlinear, lower, higher, midpoint, or nearest. For more details, see `treeple.RandomForestRegressor`.\nThe difference between the methods can be illustrated with the following example:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "interpolations = [\"linear\", \"lower\", \"higher\", \"midpoint\", \"nearest\"]\ncolors = [\"#006aff\", \"#ffd237\", \"#0d4599\", \"#f2a619\", \"#a6e5ff\"]\nquantiles = [0.025, 0.5, 0.975]\n\ny_medians = []\ny_errs = []\nest = RandomForestRegressor(\n    n_estimators=1,\n    random_state=0,\n)\n# fit the model\nest.fit(X, y)\n# get the leaf nodes that each sample fell into\nleaf_ids = est.apply(X)\n# create a list of dictionary that maps node to samples that fell into it\n# for each tree\nnode_to_indices = []\nfor tree in range(leaf_ids.shape[1]):\n    d = defaultdict(list)\n    for id, leaf in enumerate(leaf_ids[:, tree]):\n        d[leaf].append(id)\n    node_to_indices.append(d)\n# drop the X_test to the trained tree and\n# get the indices of leaf nodes that fall into it\nleaf_ids_test = est.apply(X)\n# for each samples, collect the indices of the samples that fell into\n# the same leaf node for each tree\ny_pred_quantile = []\nfor sample in range(leaf_ids_test.shape[0]):\n    li = [\n        node_to_indices[tree][leaf_ids_test[sample][tree]] for tree in range(leaf_ids_test.shape[1])\n    ]\n    # merge the list of indices into one\n    idx = [item for sublist in li for item in sublist]\n    # get the y_train for each corresponding id``\n    y_pred_quantile.append(y[idx])\n\nfor interpolation in interpolations:\n    # get the quatile preditions for each predicted sample\n    y_pred = [\n        np.array(\n            [\n                np.quantile(y_pred_quantile[i], quantile, method=interpolation)\n                for i in range(len(y_pred_quantile))\n            ]\n        )\n        for quantile in quantiles\n    ]\n    y_medians.append(y_pred[1])\n    y_errs.append(\n        np.concatenate(\n            (\n                [y_pred[1] - y_pred[0]],\n                [y_pred[2] - y_pred[1]],\n            ),\n            axis=0,\n        )\n    )\n\nsc = plt.scatter(np.arange(len(y)) - 0.35, y, color=\"k\", zorder=10)\nebs = []\nfor i, (median, y_err) in enumerate(zip(y_medians, y_errs)):\n    ebs.append(\n        plt.errorbar(\n            np.arange(len(y)) + (0.15 * (i + 1)) - 0.35,\n            median,\n            yerr=y_err,\n            color=colors[i],\n            ecolor=colors[i],\n            fmt=\"o\",\n        )\n    )\nplt.xlim([-0.75, len(y) - 0.25])\nplt.xticks(np.arange(len(y)), X.tolist())\nplt.xlabel(\"Samples (Feature Values)\")\nplt.ylabel(\"Actual and Predicted Values\")\nplt.legend([sc] + ebs, [\"actual\"] + interpolations, loc=2)\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}