{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# ExtendedIsolationForest example\n\nAn example using :class:`~treeple.ExtendedIsolationForest` for anomaly\ndetection, which compares to the :class:`~sklearn.ensemble.IsolationForest` based\non the algorithm in :footcite:`hariri2019extended`.\n\nIn the present example we demo two ways to visualize the decision boundary of an\nExtended Isolation Forest trained on a toy dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data generation\n\nWe generate two clusters (each one containing `n_samples`) by randomly\nsampling the standard normal distribution as returned by\n:func:`numpy.random.randn`. One of them is spherical and the other one is\nslightly deformed.\n\nFor consistency with the :class:`~sklearn.ensemble.IsolationForest` notation,\nthe inliers (i.e. the gaussian clusters) are assigned a ground truth label `1`\nwhereas the outliers (created with :func:`numpy.random.uniform`) are assigned\nthe label `-1`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from copy import copy\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.inspection import DecisionBoundaryDisplay\nfrom sklearn.model_selection import train_test_split\n\nfrom treeple import ExtendedIsolationForest\n\nn_samples, n_outliers = 120, 40\nrng = np.random.RandomState(0)\ncovariance = np.array([[0.5, -0.1], [0.7, 0.4]])\ncluster_1 = 0.4 * rng.randn(n_samples, 2) @ covariance + np.array([2, 2])  # general\ncluster_2 = 0.3 * rng.randn(n_samples, 2) + np.array([-2, -2])  # spherical\noutliers = rng.uniform(low=-4, high=4, size=(n_outliers, 2))\n\nX = np.concatenate([cluster_1, cluster_2, outliers])\ny = np.concatenate([np.ones((2 * n_samples), dtype=int), -np.ones((n_outliers), dtype=int)])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can visualize the resulting clusters:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scatter = plt.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor=\"k\")\nhandles, labels = scatter.legend_elements()\nplt.axis(\"square\")\nplt.legend(handles=handles, labels=[\"outliers\", \"inliers\"], title=\"true class\")\nplt.title(\"Gaussian inliers with \\nuniformly distributed outliers\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training of the model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "extended_clf = ExtendedIsolationForest(max_samples=100, random_state=0, feature_combinations=2)\nextended_clf.fit(X_train)\n\nclf = IsolationForest(max_samples=100, random_state=0)\nclf.fit(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot discrete decision boundary\n\nWe use the class :class:`~sklearn.inspection.DecisionBoundaryDisplay` to\nvisualize a discrete decision boundary. The background color represents\nwhether a sample in that given area is predicted to be an outlier\nor not. The scatter plot displays the true labels.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for name, model in zip([\"IsoForest\", \"ExtendedIsoForest\"], [clf, extended_clf]):\n    disp = DecisionBoundaryDisplay.from_estimator(\n        model,\n        X,\n        response_method=\"predict\",\n        alpha=0.5,\n    )\n    disp.ax_.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor=\"k\")\n    disp.ax_.set_title(f\"Binary decision boundary \\nof {name}\")\n    plt.axis(\"square\")\n    plt.legend(handles=handles, labels=[\"outliers\", \"inliers\"], title=\"true class\")\n    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot path length decision boundary\n\nBy setting the `response_method=\"decision_function\"`, the background of the\n:class:`~sklearn.inspection.DecisionBoundaryDisplay` represents the measure of\nnormality of an observation. Such score is given by the path length averaged\nover a forest of random trees, which itself is given by the depth of the leaf\n(or equivalently the number of splits) required to isolate a given sample.\n\nWhen a forest of random trees collectively produce short path lengths for\nisolating some particular samples, they are highly likely to be anomalies and\nthe measure of normality is close to `0`. Similarly, large paths correspond to\nvalues close to `1` and are more likely to be inliers.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for name, model in zip([\"IsoForest\", \"ExtendedIsoForest\"], [clf, extended_clf]):\n    disp = DecisionBoundaryDisplay.from_estimator(\n        model,\n        X,\n        response_method=\"decision_function\",\n        alpha=0.5,\n    )\n    disp.ax_.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor=\"k\")\n    disp.ax_.set_title(f\"Path length decision boundary \\nof {name}\")\n    plt.axis(\"square\")\n    plt.legend(handles=handles, labels=[\"outliers\", \"inliers\"], title=\"true class\")\n    plt.colorbar(disp.ax_.collections[1])\n    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate Data\nProduce 2-D dataset with a sinusoidal shape and Gaussian noise added on top.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "N = 1000\nx = np.random.rand(N) * 8 * np.pi\ny = np.sin(x) + np.random.randn(N) / 4.0\nX = np.array([x, y]).T\n\nfig = plt.figure(figsize=(6, 6))\nfig.add_subplot(111)\nplt.plot(X[:, 0], X[:, 1], \"o\", color=[0.5, 0.5, 0.5])\n\nplt.xlim([-5, 30])\nplt.ylim([-3.0, 3.0])\n\nplt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "extended_clf = ExtendedIsolationForest(\n    max_samples=100, random_state=0, n_estimators=200, feature_combinations=2\n)\nextended_clf.fit(X)\n\nclf = IsolationForest(max_samples=100, random_state=0, n_estimators=200)\nclf.fit(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot discrete decision boundary\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for name, model in zip([\"IsoForest\", \"ExtendedIsoForest\"], [clf, extended_clf]):\n    disp = DecisionBoundaryDisplay.from_estimator(\n        model,\n        X,\n        response_method=\"decision_function\",\n        alpha=0.5,\n    )\n    disp.ax_.scatter(X[:, 0], X[:, 1], c=y, s=15, edgecolor=\"k\")\n    disp.ax_.set_title(f\"Path length decision boundary \\nof {name}\")\n    plt.colorbar(disp.ax_.collections[1])\n    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize the prediction of each tree within the forest\n\nWe can visualize the prediction of each tree within the forest by using a\ncircle plot and showing the depth at which each tree isolates a given sample.\nHere, we will evaluate two samples in the sinusoidal plot: one inlier and one\noutlier. The inlier is located at the center of the sinusoidal and the outlier\nis located at the bottom right corner of the plot.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "inlier_sample = np.array([10.0, 0.0])\noutlier_sample = np.array([-5.0, -3.0])\n\nfor name, model in zip([\"IsoForest\", \"ExtendedIsoForest\"], [clf, extended_clf]):\n    theta = np.linspace(0, 2 * np.pi, len(model.estimators_))\n    max_tree_depth = max([model.estimators_[i].get_depth() for i in range(len(model.estimators_))])\n\n    fig = plt.figure()\n    ax = plt.subplot(111, projection=\"polar\")\n\n    radii_in = []\n    radii_out = []\n\n    # get the depth of each samples\n    for radii, sample, color, lw, alpha in zip(\n        [radii_in, radii_out], [inlier_sample, outlier_sample], [\"b\", \"r\"], [1, 1.3], [1, 0.9]\n    ):\n        for i in range(len(model.estimators_)):\n            # get the max depth of this tree\n            max_depth_tree = model.estimators_[i].get_depth()\n\n            leaf_index = model.estimators_[i].apply(sample.reshape(1, -1))\n            # get the depth of each tree's leaf node for this sample\n            depth = model._decision_path_lengths[i][leaf_index].squeeze()\n            radii.append(depth)\n\n        radii = np.array(radii)\n        radii = np.sort(radii) / max_tree_depth\n        for j in range(len(radii)):\n            ax.plot([theta[j], theta[j]], [0, radii[j]], color=color, alpha=alpha, lw=lw)\n\n        if color == \"b\":\n            radii_in = copy(radii)\n        else:\n            radii_out = copy(radii)\n\n    ax.set(\n        title=f\"{name}\\nNominal: Mean={np.mean(radii_in).round(3)}, \"\n        f\"Var={np.var(radii_in).round(3)}\\n\"\n        f\"Anomaly: Mean={np.mean(radii_out).round(3)}, Var={np.var(radii_out).round(3)}\",\n        xlabel=\"Anomaly\",\n    )\n    ax.set_xticklabels([])\n    ax.axes.get_xaxis().set_visible(False)\n    ax.axes.get_yaxis().set_visible(False)\n    fig.tight_layout()\n    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n\n.. footbibliography::\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}