{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Analyze a multi-view dataset with a multi-view random forest\n\nAn example using :class:`~treeple.MultiViewRandomForestClassifier` for high-dimensional\nclassification when there are multiple feature sets that are correlated with the\ntarget variable, ``y``. The multi-view random forest is a variant of the random forest\nthat samples from each feature set uniformly, instead of sampling from all features\nuniformly. This is useful when there are multiple feature sets, and some feature sets\nhave vastly different dimensionality from others.\n\nIn this case, ``X`` is a matrix of shape ``(n_samples, n_features)``, where\n``n_features`` is the sum of the number of features in each feature set. If the multi-view\nstructure is known, then one can pass this to the multi-view random forest via the\n``feature_set_ends`` parameter.\n\nFor a visualization of how the multi-view splitter works, see\n`sphx_glr_auto_examples_splitters_plot_multiview_axis_aligned_splitter.py`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import make_blobs\nfrom sklearn.model_selection import cross_val_score\n\nfrom treeple import MultiViewRandomForestClassifier, RandomForestClassifier\n\nseed = 12345\nrng = np.random.default_rng(seed)\n\n\ndef make_multiview_classification(\n    n_samples=100,\n    n_features_1=5,\n    n_features_2=1000,\n    cluster_std_first=2.0,\n    cluster_std_second=5.0,\n    X0_first=None,\n    y0=None,\n    X1_first=None,\n    y1=None,\n    seed=None,\n):\n    rng = np.random.default_rng(seed=seed)\n\n    if X0_first is None and y0 is None:\n        # Create a high-dimensional multiview dataset with a low-dimensional informative\n        # subspace in one view of the dataset.\n        X0_first, y0 = make_blobs(\n            n_samples=n_samples,\n            cluster_std=cluster_std_first,\n            n_features=n_features_1,\n            random_state=rng.integers(1, 10000),\n            centers=1,\n        )\n\n        X1_first, y1 = make_blobs(\n            n_samples=n_samples,\n            cluster_std=cluster_std_second,\n            n_features=n_features_1,\n            random_state=rng.integers(1, 10000),\n            centers=1,\n        )\n    y1[:] = 1\n    X0 = np.concatenate([X0_first, rng.standard_normal(size=(n_samples, n_features_2))], axis=1)\n    X1 = np.concatenate([X1_first, rng.standard_normal(size=(n_samples, n_features_2))], axis=1)\n    X = np.vstack((X0, X1))\n    y = np.hstack((y0, y1)).T\n\n    X = X + rng.standard_normal(size=X.shape)\n\n    return X, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simulate data\nWe simulate a 2-view dataset with both views containing informative low-dimensional features.\nThe first view has five dimensions, while the second view will vary from five to a thousand\ndimensions. The sample-size will be kept fixed, so we can compare the performance of\nregular Random forests with Multi-view Random Forests.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_samples = 500\nn_features_views = np.linspace(5, 20000, 5).astype(int)\n\ndatasets = []\n\n# make the signal portions of the dataset\nX0_first, y0 = make_blobs(\n    n_samples=n_samples,\n    cluster_std=5.0,\n    n_features=5,\n    random_state=rng.integers(1, 10000),\n    centers=1,\n)\nX1_first, y1 = make_blobs(\n    n_samples=n_samples,\n    cluster_std=10.0,\n    n_features=5,\n    random_state=rng.integers(1, 10000),\n    centers=1,\n)\n\n# increasingly add noise dimensions to the second view\nfor idx, n_features in enumerate(n_features_views):\n    X, y = make_multiview_classification(\n        n_samples=n_samples,\n        n_features_1=5,\n        n_features_2=n_features,\n        cluster_std_first=5.0,\n        cluster_std_second=10.0,\n        # X0_first=X0_first, y0=y0,\n        # X1_first=X1_first, y1=y1,\n        seed=seed + idx,\n    )\n    datasets.append((X, y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fit Random Forest and Multi-view Random Forest\nHere, we fit both forests over all the datasets.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_estimators = 100\nn_jobs = -1\nmax_features = \"sqrt\"\n\nscores = defaultdict(list)\n\nfor idx, ((X, y), n_features) in enumerate(zip(datasets, n_features_views)):\n    feature_set_ends = np.array([5, n_features + 5])\n\n    rf = RandomForestClassifier(\n        n_estimators=n_estimators,\n        n_jobs=n_jobs,\n        random_state=seed,\n        max_features=max_features,\n    )\n\n    mvrf = MultiViewRandomForestClassifier(\n        n_estimators=n_estimators,\n        n_jobs=n_jobs,\n        feature_set_ends=feature_set_ends,\n        random_state=seed,\n        max_features=max_features,\n    )\n\n    # obtain the cross-validation score\n    rf_scores = cross_val_score(rf, X, y, cv=3)\n    mvrf_scores = cross_val_score(mvrf, X, y, cv=3)\n\n    scores[\"rf\"].extend(rf_scores)\n    scores[\"mvrf\"].extend(mvrf_scores)\n    scores[\"n_features\"].extend([n_features + 5] * len(rf_scores))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize scores and compare performance\nNow, we can compare the performance from the cross-validation experiment.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(scores)\n\n# melt the dataframe, to make it easier to plot\ndf = pd.melt(df, id_vars=[\"n_features\"], var_name=\"model\", value_name=\"score\")\n\nfig, ax = plt.subplots()\nsns.lineplot(data=df, x=\"n_features\", y=\"score\", marker=\"o\", hue=\"model\", ax=ax)\nax.set_ylabel(\"CV Score\")\nax.set_xlabel(\"Number of features in second view\")\nax.set_title(\"Random Forest vs Multi-view Random Forest\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see, the multi-view random forest outperforms the regular random forest\nas the number of features in the second view increases. This is because the multi-view\nrandom forest samples from each feature-set uniformly, while the regular random forest\nsamples from all features uniformly. This is a key difference between the two forests.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}