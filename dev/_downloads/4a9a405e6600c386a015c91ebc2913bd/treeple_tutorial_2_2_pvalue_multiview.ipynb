{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Calculating p-value with multiview data (CoMIGHT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nfrom treeple.datasets import make_trunk_classification\nfrom treeple.ensemble import HonestForestClassifier\nfrom treeple.stats import PermutationHonestForestClassifier, build_coleman_forest\nfrom treeple.tree import MultiViewDecisionTreeClassifier\n\nsns.set(color_codes=True, style=\"white\", context=\"talk\", font_scale=1.5)\nPALETTE = sns.color_palette(\"Set1\")\nsns.set_palette(PALETTE[1:5] + PALETTE[6:], n_colors=9)\nsns.set_style(\"white\", {\"axes.edgecolor\": \"#dddddd\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Independence Testing\n\nGiven samples from ``X``, ``Z``, and ``Y``, the independent hypothesis\nand its alternative are stated as:\n\n\\begin{align}H_0 : F_{X,Y|Z} = F_{X|Z} F_{Y|Z}\\end{align}\n\n\\begin{align}H_A : F_{X,Y|Z} \\neq F_{X|Z} F_{Y|Z}\\end{align}\n\nBy computing the p-value using ``treeple``, we can test if $H_0$\nwould be rejected, which confirms that ``X|Z`` and ``Y|Z`` are not independent.\nThe p-value is generated by comparing the observed\nstatistic difference with permuted differences, using conditional mutual\ninformation as a test statistic in this example.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CMI\n\nConditional mutual information (*CMI*) measures the dependence of *Y* on\n*X* conditioned on *Z*. It can be calculated by the difference between\nthe joint *MI* (``I([X, Z]; Y)``) and the *MI* of Y on Z (``I(Y; Z)``):\n\n\\begin{align}I(Y; X \\mid Z) = I(Y; [X, Z]) - I(Y; Z)\\end{align}\n\nUnder the null hypothesis $H_0$, the joint *MI* ``I(Y; [X, Z])``\nis equal to the *MI* of Y on Z ``I(Y; Z)``, so the *CMI* becomes zero. Thus, if\nthe *CMI* is significantly larger than zero, we can reject the null hypothesis\n$H_0$.\n\nWith a multiview binary class simulation as an example, this tutorial\nwill show how to use ``treeple`` to calculate the statistic and test the\nhypothesis with multiview data.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create a simulation with two gaussians\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# create a binary class simulation with two gaussians\n# 500 samples for each class, class zero is standard\n# gaussian, and class one has a mean at one for Z\nZ, y = make_trunk_classification(\n    n_samples=1000,\n    n_dim=1,\n    mu_0=0,\n    mu_1=1,\n    n_informative=1,\n    seed=1,\n)\n\n# class one has a mean at two for X\nX, y = make_trunk_classification(\n    n_samples=1000,\n    n_dim=1,\n    mu_0=0,\n    mu_1=2,\n    n_informative=1,\n    seed=2,\n)\n\nZ_X = np.hstack((Z, X))\n\n\nZ_X_y = np.hstack((Z_X, y.reshape(-1, 1)))\nZ_X_y = pd.DataFrame(Z_X_y, columns=[\"Z\", \"X\", \"y\"])\nZ_X_y = Z_X_y.replace({\"y\": 0.0}, \"Class Zero\")\nZ_X_y = Z_X_y.replace({\"y\": 1.0}, \"Class One\")\n\nfig, ax = plt.subplots(figsize=(6, 6))\nfig.tight_layout()\nax.tick_params(labelsize=15)\nsns.scatterplot(data=Z_X_y, x=\"Z\", y=\"X\", hue=\"y\", palette=PALETTE[:2][::-1], alpha=0.2)\nsns.kdeplot(data=Z_X_y, x=\"Z\", y=\"X\", hue=\"y\", palette=PALETTE[:2][::-1], alpha=0.6)\nax.set_ylabel(\"Variable Two\", fontsize=15)\nax.set_xlabel(\"Variable One\", fontsize=15)\nplt.legend(frameon=False, fontsize=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Modify the simulation with permuted variable two\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# shuffle the labels\nX_null = np.copy(X)\nnp.random.shuffle(X_null)\n\nZ_X_y.insert(2, \"Permuted X\", X_null)\n\nfig, ax = plt.subplots(figsize=(6, 6))\nfig.tight_layout()\nax.tick_params(labelsize=15)\nsns.scatterplot(data=Z_X_y, x=\"Z\", y=\"Permuted X\", hue=\"y\", palette=PALETTE[:2][::-1], alpha=0.2)\nsns.kdeplot(data=Z_X_y, x=\"Z\", y=\"Permuted X\", hue=\"y\", palette=PALETTE[:2][::-1], alpha=0.6)\nax.set_ylabel(\"Permuted Variable Two\", fontsize=15)\nax.set_xlabel(\"Variable One\", fontsize=15)\nplt.legend(frameon=False, fontsize=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fit the models and calculate the p-value\n\nConstruct two forests, one trained with original data,\nand the other trained with permuted data. The test randomly\npermutes the two forests for a specified number of times.\n\nEach pair of forest outputs a set of mutual information statistics,\nand the statistic differences are used to calculate the p-vale.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# initialize the forest with 100 trees\nest = HonestForestClassifier(\n    n_estimators=100,\n    max_samples=1.6,\n    max_features=0.3,\n    bootstrap=True,\n    stratify=True,\n    tree_estimator=MultiViewDecisionTreeClassifier(),\n    random_state=1,\n)\n\n# initialize another forest with 100 trees\nest_null = PermutationHonestForestClassifier(\n    n_estimators=100,\n    max_samples=1.6,\n    max_features=0.3,\n    bootstrap=True,\n    stratify=True,\n    tree_estimator=MultiViewDecisionTreeClassifier(),\n    random_state=1,\n)\n\nPERMUTE = 10000\n\n# conduct the hypothesis test with conditional mutual information\nobserved_diff, _, _, pvalue, mix_diff = build_coleman_forest(\n    est,\n    est_null,\n    Z_X,\n    y,\n    metric=\"mi\",\n    n_repeats=PERMUTE,\n    covariate_index=[1],\n    return_posteriors=False,\n    seed=1,\n)\n\nfig, ax = plt.subplots(figsize=(6, 6))\nfig.tight_layout()\nax.tick_params(labelsize=15)\n\n# histogram plot the statistic differences\nax.hist(mix_diff, bins=50, alpha=0.6, color=PALETTE[1], label=\"null\")\nax.axvline(x=observed_diff, color=PALETTE[0], linestyle=\"--\", label=\"observed\")\nax.set_xlabel(\"Conditional Mutual Information Diff\", fontsize=15)\nax.set_ylabel(\"# of Samples\", fontsize=15)\nplt.legend(frameon=False, fontsize=15)\nplt.show()\n\nprint(\"p-value is:\", round(pvalue, 2))\nif pvalue < 0.05:\n    print(\"The null hypothesis is rejected.\")\nelse:\n    print(\"The null hypothesis is not rejected.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}