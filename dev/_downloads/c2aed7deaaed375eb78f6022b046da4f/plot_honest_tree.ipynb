{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Comparison of Decision Tree and Honest Tree\n\nThis example compares the :class:`treeple.tree.HonestTreeClassifier` from the\n``treeple`` library with the :class:`sklearn.tree.DecisionTreeClassifier`\nfrom scikit-learn on the Iris dataset.\n\nBoth classifiers are fitted on the same dataset and their decision trees\nare plotted side by side.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nfrom sklearn import config_context\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\n\nfrom treeple.tree import HonestTreeClassifier\n\n# Load the iris dataset\niris = load_iris()\nX, y = iris.data, iris.target\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=0)\n\n# Initialize classifiers\nmax_features = 0.3\n\ndishonest_clf = HonestTreeClassifier(\n    honest_method=None,\n    max_features=max_features,\n    random_state=0,\n    honest_prior=\"ignore\",\n)\nhonest_noprune_clf = HonestTreeClassifier(\n    honest_method=\"apply\",\n    max_features=max_features,\n    random_state=0,\n    honest_prior=\"ignore\",\n)\nhonest_clf = HonestTreeClassifier(honest_method=\"prune\", max_features=max_features, random_state=0)\nsklearn_clf = DecisionTreeClassifier(max_features=max_features, random_state=0)\n\n# Fit classifiers\ndishonest_clf.fit(X_train, y_train)\nhonest_noprune_clf.fit(X_train, y_train)\nhonest_clf.fit(X_train, y_train)\nsklearn_clf.fit(X_train, y_train)\n\n# Plotting the trees\nfig, axes = plt.subplots(nrows=1, ncols=4, figsize=(15, 5))\n\n# .. note:: We skip parameter validation because internally the `plot_tree`\n#           function checks if the estimator is a DecisionTreeClassifier\n#           instance from scikit-learn, but the ``HonestTreeClassifier`` is\n#           a subclass of a forked version of the DecisionTreeClassifier.\n\n# Plot HonestTreeClassifier tree\nax = axes[2]\nwith config_context(skip_parameter_validation=True):\n    plot_tree(honest_clf, filled=True, ax=ax)\nax.set_title(\"HonestTreeClassifier\")\n\n# Plot HonestTreeClassifier tree\nax = axes[1]\nwith config_context(skip_parameter_validation=True):\n    plot_tree(honest_noprune_clf, filled=False, ax=ax)\nax.set_title(\"HonestTreeClassifier (No pruning)\")\n\n# Plot HonestTreeClassifier tree\nax = axes[0]\nwith config_context(skip_parameter_validation=True):\n    plot_tree(dishonest_clf, filled=False, ax=ax)\nax.set_title(\"HonestTreeClassifier (Dishonest)\")\n\n\n# Plot scikit-learn DecisionTreeClassifier tree\nplot_tree(sklearn_clf, filled=True, ax=axes[3])\naxes[3].set_title(\"DecisionTreeClassifier\")\n\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Discussion\nThe HonestTreeClassifier is a variant of the DecisionTreeClassifier that\nprovides honest inference. The honest inference is achieved by splitting the\ndataset into two parts: the training set and the validation set. The training\nset is used to build the tree, while the validation set is used to fit the\nleaf nodes for posterior prediction. This results in calibrated posteriors\n(see `sphx_glr_auto_examples_calibration_plot_overlapping_gaussians.py`).\n\nCompared to the ``honest_prior='apply'`` method, the ``honest_prior='prune'``\nmethod builds a tree that will not contain empty leaves, and also leverages\nthe validation set to check split conditions. Thus we see that the pruned\nhonest tree is significantly smaller than the regular decision tree.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate predictions of the trees\nWhen we do not prune, note that the honest tree will have empty leaves\nthat predict the prior. In this case, ``honest_prior='ignore'`` is used\nto ignore these leaves when computing the posteriors, which will result\nin a posterior that is ``np.nan``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# this is the same as a decision tree classifier that is trained on less data\nprint(\"\\nDishonest posteriors: \", dishonest_clf.predict_proba(X_val))\n\n# this is the honest tree with empty leaves that predict the prior\nprint(\"\\nHonest tree without pruning: \", honest_noprune_clf.predict_proba(X_val))\n\n# this is the honest tree that is pruned\nprint(\"\\nHonest tree with pruning: \", honest_clf.predict_proba(X_val))\n\n# this is a regular decision tree classifier from sklearn\nprint(\"\\nDTC: \", sklearn_clf.predict_proba(X_val))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}